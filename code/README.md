# Robust Ensemble Knowledge Distillation

Extending AGRE-KD with class labels and feature distillation for improved group robustness.

## ğŸš€ Quick Start (Google Colab)

**Recommended**: Use the complete Colab notebook: `notebooks/colab_full_workflow.ipynb`

This notebook handles everything: setup, data download, teacher preparation, and experiments.

### Manual Setup

```bash
# 1. Clone and setup
git clone https://github.com/YOUR_USERNAME/robust-ensemble-kd.git
cd robust-ensemble-kd/code
pip install torch torchvision wilds tqdm pandas numpy matplotlib scikit-learn

# 2. Download Waterbirds data
python -c "from data import download_waterbirds; download_waterbirds('./data')"

# 3. Download DFR checkpoints to teacher_checkpoints/ (see below)

# 4. Prepare debiased teachers (saves to same folder)
python prepare_teachers.py \
    --checkpoint_dir ./teacher_checkpoints \
    --data_dir ./data/waterbirds_v1.0

# 5. Run experiments
python train.py --mode student \
    --data_dir ./data/waterbirds_v1.0 \
    --teacher_paths ./teacher_checkpoints/teacher_*_debiased.pt \
    --alpha 0.7 --gamma 0.1 --exp_name exp3_combined
```

## ğŸ“ Project Structure

```
code/
â”œâ”€â”€ data.py              # Waterbirds dataloader with group labels
â”œâ”€â”€ models.py            # ResNet with feature extraction hooks
â”œâ”€â”€ losses.py            # KD, feature, AGRE-KD losses
â”œâ”€â”€ eval.py              # WGA and per-group accuracy metrics
â”œâ”€â”€ config.py            # All hyperparameters
â”œâ”€â”€ train.py             # Teacher and student training loops
â”œâ”€â”€ dfr.py               # Deep Feature Reweighting implementation
â”œâ”€â”€ prepare_teachers.py  # Prepare debiased teachers from ERM checkpoints
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ notebooks/
    â”œâ”€â”€ colab_full_workflow.ipynb  # â­ Complete Colab workflow
    â””â”€â”€ run_experiments.ipynb      # Alternative experiment runner
```

## ğŸ“‹ Complete Colab Workflow

### Step 1: Setup (5 min)

```python
# Mount Drive & install deps
from google.colab import drive
drive.mount('/content/drive')
!pip install -q wilds tqdm scikit-learn

# Clone repo
!git clone https://github.com/YOUR_USERNAME/robust-ensemble-kd.git /content/repo
%cd /content/repo/code
```

### Step 2: Download DFR Checkpoints (~5 min)

The DFR authors provide pre-trained ERM checkpoints:

**Source**: https://drive.google.com/drive/folders/1OQ_oPPgxgK_7j_GCt71znyiRj6hqi_UW

1. Navigate to: `spurious_feature_learning/results/waterbirds_paper`
2. Download 3-5 checkpoint files (e.g., `erm_seed0.pt`, `erm_seed1.pt`, etc.)
3. Upload to your Google Drive: `MyDrive/robust-ensemble-kd/teacher_checkpoints/`

### Step 3: Prepare Teachers (~30 min)

```python
from prepare_teachers import colab_prepare_teachers

results = colab_prepare_teachers(
    checkpoint_dir='/content/drive/MyDrive/robust-ensemble-kd/teacher_checkpoints',
    data_dir='/content/drive/MyDrive/robust-ensemble-kd/data/waterbirds_v1.0',
    num_teachers=5
)
```

This adds debiased teachers to the same folder:
- `erm_seed0.pt` ... `erm_seed4.pt` â†’ Biased (~70% WGA) - **you download these**
- `teacher_0_debiased.pt` ... `teacher_4_debiased.pt` â†’ Debiased (~92% WGA) - **created by DFR**

### Step 4: Run Experiments (~2-3 hrs each)

```python
from config import Config
from train import train_student

# Load teachers
teachers = load_teachers(...)  # See notebook for details

# Baseline
config = Config(alpha=1.0, gamma=0.0, epochs=30)
train_student(config, teachers, exp_name='baseline')

# Experiment 1: Add class labels
config = Config(alpha=0.7, gamma=0.0, epochs=30)
train_student(config, teachers, exp_name='exp1_alpha07')

# Experiment 2: Feature distillation  
config = Config(alpha=1.0, gamma=0.1, epochs=30)
train_student(config, teachers, exp_name='exp2_gamma01')

# Experiment 3: Combined
config = Config(alpha=0.7, gamma=0.1, epochs=30)
train_student(config, teachers, exp_name='exp3_combined')
```

## ğŸ”§ Usage Examples

### Load Data

```python
from data import get_waterbirds_loaders

loaders = get_waterbirds_loaders(
    root_dir='./data/waterbirds_v1.0',
    batch_size=32,
    augment=True
)
# loaders['train'], loaders['val'], loaders['test']
```

### Create Models

```python
from models import get_teacher_model, get_student_model, create_feature_adapter

# Teacher (ResNet-50)
teacher = get_teacher_model('resnet50', num_classes=2, pretrained=True)

# Student (ResNet-18)  
student = get_student_model('resnet18', num_classes=2, pretrained=True)

# Adapter for feature distillation (512 -> 2048)
adapter = create_feature_adapter('resnet18', 'resnet50', 'pooled')
```

### Extract Features

```python
# Forward pass with features
logits, features = teacher(images, return_features=True)
# features['pooled'] = penultimate features [B, 2048]
# features['layer4'] = spatial features [B, 2048, 7, 7]
```

### Apply DFR (Debiasing)

```python
from dfr import apply_dfr

# Transform biased model (~70% WGA) to debiased (~92% WGA)
apply_dfr(model, val_loader, device='cuda', method='sklearn', balance_type='group')
```

### Compute Losses

```python
from losses import AGREKDLoss

# AGRE-KD with class labels and features
loss_fn = AGREKDLoss(alpha=0.7, gamma=0.1, temperature=4.0,
                      student_dim=512, teacher_dim=2048)

# Compute teacher weights (gradient-based)
weights = loss_fn.compute_teacher_weights(student, teacher_logits_list, 
                                          biased_logits, student_logits)

# Compute loss
loss, loss_dict = loss_fn(student_logits, teacher_logits_list, labels,
                          student_features, teacher_features_list,
                          teacher_weights=weights)
```

### Evaluate

```python
from eval import compute_group_accuracies, print_results

results = compute_group_accuracies(model, loaders['test'])
print_results(results)
# Shows: per-group acc, WGA, average acc, accuracy gap
```

## ğŸ“Š Expected Results

| Method | Waterbirds WGA | Avg Acc |
|--------|----------------|---------|
| ERM (single model) | 68-72% | 97% |
| Deep Ensemble (3 models) | 75-80% | 96% |
| DFR | 91-93% | 94% |
| AGRE-KD (paper) | ~85-88% | 92% |
| **Your target** | â‰¥85% | â‰¥90% |

## ğŸ”¬ Your Three Experiments

### Experiment 1: Class Labels (Î± < 1, Î³ = 0)

**Hypothesis**: Adding ground-truth supervision alongside KD helps when teachers make mistakes.

```python
config = Config(alpha=0.7, gamma=0.0)  # Test Î± âˆˆ {0.5, 0.7, 0.9}
```

### Experiment 2: Feature Distillation (Î± = 1, Î³ > 0)

**Hypothesis**: Distilling penultimate features transfers more robust representations.

```python
config = Config(alpha=1.0, gamma=0.1)  # Test Î³ âˆˆ {0.1, 0.25}
```

### Experiment 3: Combined (Î± < 1, Î³ > 0)

**Hypothesis**: Both modifications together provide complementary benefits.

```python
config = Config(alpha=0.7, gamma=0.1)
```

## ğŸ’¾ Google Drive Structure

```
MyDrive/robust-ensemble-kd/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ waterbirds_v1.0/           # Dataset (downloaded via WILDS)
â”œâ”€â”€ teacher_checkpoints/            # All teachers in ONE folder
â”‚   â”œâ”€â”€ erm_seed0.pt               # Biased (downloaded from DFR)
â”‚   â”œâ”€â”€ erm_seed1.pt               # Biased (downloaded from DFR)
â”‚   â”œâ”€â”€ erm_seed2.pt               # Biased (downloaded from DFR)
â”‚   â”œâ”€â”€ teacher_0_debiased.pt      # Debiased (created by prepare_teachers.py)
â”‚   â”œâ”€â”€ teacher_1_debiased.pt      # Debiased (created by prepare_teachers.py)
â”‚   â”œâ”€â”€ teacher_2_debiased.pt      # Debiased (created by prepare_teachers.py)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints/                    # Student training checkpoints
â”‚   â”œâ”€â”€ student_baseline_best.pt
â”‚   â”œâ”€â”€ student_exp1_alpha07_best.pt
â”‚   â””â”€â”€ ...
â””â”€â”€ logs/
    â””â”€â”€ experiment_results.json
```

## ğŸš¨ Troubleshooting

**Colab disconnects**: Checkpoints saved every 5 epochs. Re-run cells 1-4, then resume.

**OOM errors**: Reduce `batch_size` to 16, use `torch.cuda.empty_cache()`

**Slow data loading**: Copy data to Colab local storage:
```bash
!cp -r /content/drive/MyDrive/.../data /content/data_local
```

**WILDS download fails**: Download manually from:
https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz

**DFR checkpoints not found**: Download from:
https://drive.google.com/drive/folders/1OQ_oPPgxgK_7j_GCt71znyiRj6hqi_UW

## â±ï¸ Time Estimates (T4 GPU)

| Task | Time |
|------|------|
| Setup & data download | 10 min |
| Prepare 5 teachers (DFR) | 30 min |
| Student training (30 epochs) | 2-3 hrs |
| Full experiment suite (6 configs) | ~15-20 hrs |

## ğŸ“š References

- AGRE-KD: [arXiv:2411.14984](https://arxiv.org/abs/2411.14984)
- DFR: [arXiv:2204.02937](https://arxiv.org/abs/2204.02937) 
- DFR Checkpoints: [Google Drive](https://drive.google.com/drive/folders/1OQ_oPPgxgK_7j_GCt71znyiRj6hqi_UW)
- Group DRO: [github.com/kohpangwei/group_DRO](https://github.com/kohpangwei/group_DRO)
- WILDS: [github.com/p-lambda/wilds](https://github.com/p-lambda/wilds)
