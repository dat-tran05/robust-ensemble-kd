{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7BNfijChYKp"
   },
   "source": [
    "# üöÄ Setup (Run Every Session)\n",
    "\n",
    "Run this cell every time you open the notebook to:\n",
    "- Mount Google Drive\n",
    "- Install dependencies\n",
    "- Clone/update the code repository\n",
    "- Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KlmeIlihYKq",
    "outputId": "f6bb46da-7b01-442a-92eb-db66a71d4ac3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.2/126.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCloning repo...\n",
      "Cloning into '/content/repo'...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 36 (delta 13), reused 31 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (36/36), 59.44 KiB | 9.91 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n",
      "\n",
      "‚úÖ Setup complete!\n",
      "   Data: /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/data/waterbirds_v1.0\n",
      "   Teachers: /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# === SETUP (Run Every Session) ===\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Install dependencies\n",
    "!pip install -q wilds tqdm scikit-learn\n",
    "\n",
    "# 3. Clone or update repo\n",
    "REPO_DIR = '/content/repo'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(\"Repo exists, pulling latest...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    print(\"Cloning repo...\")\n",
    "    !git clone https://github.com/dat-tran05/robust-ensemble-kd.git {REPO_DIR}\n",
    "\n",
    "# 4. Add to Python path\n",
    "sys.path.insert(0, f'{REPO_DIR}/light-code')\n",
    "\n",
    "# 5. Define paths (EDIT THIS IF YOUR DRIVE PATH IS DIFFERENT)\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd'\n",
    "DATA_DIR = f'{DRIVE_ROOT}/data/waterbirds_v1.0'\n",
    "TEACHER_DIR = f'{DRIVE_ROOT}/teacher_checkpoints'\n",
    "CHECKPOINT_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Teachers: {TEACHER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MN9Vp-lhYKq"
   },
   "source": [
    "# üìä Load Data & Verify\n",
    "\n",
    "Load the Waterbirds dataset and verify that teacher checkpoints exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kj_UF1qThYKq",
    "outputId": "89adf279-dc3f-4c7e-c789-85ffaa3a2876"
   },
   "outputs": [],
   "source": "# === LOAD DATA & VERIFY ===\nimport torch\nfrom data import get_waterbirds_loaders\nfrom models import get_teacher_model, load_teacher_checkpoint\nfrom eval import compute_group_accuracies, print_results\n\n# Load data\nprint(\"Loading Waterbirds data...\")\nloaders = get_waterbirds_loaders(DATA_DIR, batch_size=32, num_workers=4)\n\n# Check teacher checkpoints\nprint(f\"\\nTeacher checkpoints in {TEACHER_DIR}:\")\nckpts = sorted([f for f in os.listdir(TEACHER_DIR) if f.endswith('.pt')])\nfor f in ckpts:\n    print(f\"  - {f}\")\n\n# Quick sanity check\nprint(\"\\nQuick sanity check...\")\nteacher = get_teacher_model('resnet50', num_classes=2, pretrained=False)\nload_teacher_checkpoint(teacher, os.path.join(TEACHER_DIR, ckpts[0]))\nteacher.cuda().eval()\n\nbatch = next(iter(loaders['test']))\nwith torch.no_grad():\n    preds = teacher(batch['image'].cuda()).argmax(dim=1)\n    acc = (preds.cpu() == batch['label']).float().mean()\nprint(f\"Test batch accuracy: {acc*100:.1f}%\")\nprint(\"‚úÖ Everything loaded correctly!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFXK57MxoSJ_"
   },
   "source": [
    "# üìà Evaluate Baseline Teacher\n",
    "\n",
    "Evaluate the original ERM teacher on the full test set to see baseline performance.\n",
    "Expected: ~73% WGA (before DFR debiasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43BZBUEGoRwe",
    "outputId": "2daa3765-6b31-4b83-b8e7-2fae1b71005f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline (biased) teacher on full test set...\n",
      "Using: erm_seed1.pt\n",
      "\n",
      "  Evaluating 5794 samples (182 batches)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation complete (416.6s) - WGA: 73.8%\n",
      "\n",
      "==================================================\n",
      " Baseline Teacher: erm_seed1.pt\n",
      "==================================================\n",
      "\n",
      "Per-group accuracy:\n",
      "  Landbird + Land (majority): 99.51% (n=2255)\n",
      "  Landbird + Water (minority): 87.89% (n=2255)\n",
      "  Waterbird + Land (minority, hardest): 73.83% (n=642)\n",
      "  Waterbird + Water (majority): 96.42% (n=642)\n",
      "\n",
      "Aggregate metrics:\n",
      "  Worst-Group Accuracy (WGA): 73.83%\n",
      "  Average Accuracy: 91.80%\n",
      "  Accuracy Gap: 25.68%\n",
      "  Worst Group: 2\n",
      "==================================================\n",
      "\n",
      "\n",
      "üìä Baseline WGA: 73.83%\n",
      "   (This should improve to ~90%+ after DFR)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# === EVALUATE BASELINE TEACHER ===\n",
    "print(\"Evaluating baseline (biased) teacher on full test set...\")\n",
    "print(f\"Using: {ckpts[0]}\\n\")\n",
    "\n",
    "baseline_results = compute_group_accuracies(teacher, loaders['test'], device='cuda')\n",
    "print_results(baseline_results, f\"Baseline Teacher: {ckpts[0]}\")\n",
    "\n",
    "print(f\"\\nüìä Baseline WGA: {baseline_results['wga']*100:.2f}%\")\n",
    "print(\"   (This should improve to ~90%+ after DFR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m3UhXRehYKr"
   },
   "source": [
    "# üîß Apply DFR to Teachers\n",
    "\n",
    "Apply Deep Feature Reweighting to create debiased versions of the ERM teachers.\n",
    "- Takes ~30 minutes for 5 teachers\n",
    "- Only need to run this ONCE (results saved to Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FH19FHEIhYKr",
    "outputId": "1836e1d9-ac86-41a9-d3c4-5af49e1d3cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying DFR to all ERM teachers...\n",
      "This will create debiased versions in: /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints\n",
      "\n",
      "\n",
      "============================================================\n",
      "PREPARING 5 TEACHERS\n",
      "============================================================\n",
      "\n",
      "Found 5 ERM checkpoints:\n",
      "  - erm_seed1.pt (seed 1)\n",
      "  - erm_seed2.pt (seed 2)\n",
      "  - erm_seed3.pt (seed 3)\n",
      "  - erm_seed4.pt (seed 4)\n",
      "  - erm_seed5.pt (seed 5)\n",
      "\n",
      "Loading data...\n",
      "Loaded train split: 4795 samples\n",
      "  Group counts: {0: np.int64(3498), 1: np.int64(184), 2: np.int64(56), 3: np.int64(1057)}\n",
      "  Worst group: 2 with 56 samples\n",
      "Loaded val split: 1199 samples\n",
      "  Group counts: {0: np.int64(467), 1: np.int64(466), 2: np.int64(133), 3: np.int64(133)}\n",
      "  Worst group: 2 with 133 samples\n",
      "Loaded test split: 5794 samples\n",
      "  Group counts: {0: np.int64(2255), 1: np.int64(2255), 2: np.int64(642), 3: np.int64(642)}\n",
      "  Worst group: 2 with 642 samples\n",
      "Data loaded (0.1s)\n",
      "\n",
      "============================================================\n",
      "[1/5] Processing erm_seed1.pt\n",
      "============================================================\n",
      "  [1/5] Loading model...\n",
      "Loaded checkpoint from /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/erm_seed1.pt\n",
      "  All keys matched!\n",
      "        Done (0.6s)\n",
      "  [2/5] Evaluating biased (ERM) model...\n",
      "        WGA: 73.8% (43.0s)\n",
      "  [3/5] Applying DFR (retraining last layer)...\n",
      "      [1/3] Extracting features from 1199 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features: (1199, 2048) (9.9s)\n",
      "      [2/3] Creating balanced subset...\n",
      "            532 samples (0.0s)\n",
      "      [3/3] Training new classifier...\n",
      "            Done (0.1s)\n",
      "        Done (10.0s)\n",
      "  [4/5] Evaluating debiased (DFR) model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        WGA: 93.3% (43.7s)\n",
      "  [5/5] Saving checkpoint...\n",
      "        Saved: teacher_1_debiased.pt (0.3s)\n",
      "\n",
      "  Summary: 73.8% -> 93.3% (+19.5%) in 97.6s\n",
      "\n",
      "[1/5] Done in 97.6s (elapsed: 1.6min, remaining: ~6.5min)\n",
      "\n",
      "============================================================\n",
      "[2/5] Processing erm_seed2.pt\n",
      "============================================================\n",
      "  [1/5] Loading model...\n",
      "Loaded checkpoint from /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/erm_seed2.pt\n",
      "  All keys matched!\n",
      "        Done (0.6s)\n",
      "  [2/5] Evaluating biased (ERM) model...\n",
      "        WGA: 71.2% (44.5s)\n",
      "  [3/5] Applying DFR (retraining last layer)...\n",
      "      [1/3] Extracting features from 1199 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features: (1199, 2048) (10.1s)\n",
      "      [2/3] Creating balanced subset...\n",
      "            532 samples (0.0s)\n",
      "      [3/3] Training new classifier...\n",
      "            Done (0.1s)\n",
      "        Done (10.2s)\n",
      "  [4/5] Evaluating debiased (DFR) model...\n",
      "        WGA: 92.1% (43.9s)\n",
      "  [5/5] Saving checkpoint...\n",
      "        Saved: teacher_2_debiased.pt (0.2s)\n",
      "\n",
      "  Summary: 71.2% -> 92.1% (+20.9%) in 99.4s\n",
      "\n",
      "[2/5] Done in 99.4s (elapsed: 3.3min, remaining: ~4.9min)\n",
      "\n",
      "============================================================\n",
      "[3/5] Processing erm_seed3.pt\n",
      "============================================================\n",
      "  [1/5] Loading model...\n",
      "Loaded checkpoint from /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/erm_seed3.pt\n",
      "  All keys matched!\n",
      "        Done (3.1s)\n",
      "  [2/5] Evaluating biased (ERM) model...\n",
      "        WGA: 69.6% (44.2s)\n",
      "  [3/5] Applying DFR (retraining last layer)...\n",
      "      [1/3] Extracting features from 1199 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features: (1199, 2048) (9.1s)\n",
      "      [2/3] Creating balanced subset...\n",
      "            532 samples (0.0s)\n",
      "      [3/3] Training new classifier...\n",
      "            Done (0.2s)\n",
      "        Done (9.3s)\n",
      "  [4/5] Evaluating debiased (DFR) model...\n",
      "        WGA: 91.9% (43.4s)\n",
      "  [5/5] Saving checkpoint...\n",
      "        Saved: teacher_3_debiased.pt (0.2s)\n",
      "\n",
      "  Summary: 69.6% -> 91.9% (+22.3%) in 100.2s\n",
      "\n",
      "[3/5] Done in 100.2s (elapsed: 5.0min, remaining: ~3.3min)\n",
      "\n",
      "============================================================\n",
      "[4/5] Processing erm_seed4.pt\n",
      "============================================================\n",
      "  [1/5] Loading model...\n",
      "Loaded checkpoint from /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/erm_seed4.pt\n",
      "  All keys matched!\n",
      "        Done (3.8s)\n",
      "  [2/5] Evaluating biased (ERM) model...\n",
      "        WGA: 70.9% (43.4s)\n",
      "  [3/5] Applying DFR (retraining last layer)...\n",
      "      [1/3] Extracting features from 1199 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features: (1199, 2048) (9.6s)\n",
      "      [2/3] Creating balanced subset...\n",
      "            532 samples (0.0s)\n",
      "      [3/3] Training new classifier...\n",
      "            Done (0.2s)\n",
      "        Done (9.8s)\n",
      "  [4/5] Evaluating debiased (DFR) model...\n",
      "        WGA: 93.6% (43.9s)\n",
      "  [5/5] Saving checkpoint...\n",
      "        Saved: teacher_4_debiased.pt (0.3s)\n",
      "\n",
      "  Summary: 70.9% -> 93.6% (+22.7%) in 101.2s\n",
      "\n",
      "[4/5] Done in 101.2s (elapsed: 6.6min, remaining: ~1.7min)\n",
      "\n",
      "============================================================\n",
      "[5/5] Processing erm_seed5.pt\n",
      "============================================================\n",
      "  [1/5] Loading model...\n",
      "Loaded checkpoint from /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/erm_seed5.pt\n",
      "  All keys matched!\n",
      "        Done (3.9s)\n",
      "  [2/5] Evaluating biased (ERM) model...\n",
      "        WGA: 68.7% (43.4s)\n",
      "  [3/5] Applying DFR (retraining last layer)...\n",
      "      [1/3] Extracting features from 1199 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Features: (1199, 2048) (9.7s)\n",
      "      [2/3] Creating balanced subset...\n",
      "            532 samples (0.0s)\n",
      "      [3/3] Training new classifier...\n",
      "            Done (0.1s)\n",
      "        Done (9.8s)\n",
      "  [4/5] Evaluating debiased (DFR) model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        WGA: 93.8% (42.9s)\n",
      "  [5/5] Saving checkpoint...\n",
      "        Saved: teacher_5_debiased.pt (0.2s)\n",
      "\n",
      "  Summary: 68.7% -> 93.8% (+25.1%) in 100.2s\n",
      "\n",
      "[5/5] Done in 100.2s (elapsed: 8.3min, remaining: ~0.0min)\n",
      "\n",
      "============================================================\n",
      "TEACHER PREPARATION COMPLETE (8.3 min)\n",
      "============================================================\n",
      "\n",
      "All teachers in: /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints\n",
      "\n",
      "Biased (ERM):\n",
      "  erm_seed1.pt -> WGA=73.8%\n",
      "  erm_seed2.pt -> WGA=71.2%\n",
      "  erm_seed3.pt -> WGA=69.6%\n",
      "  erm_seed4.pt -> WGA=70.9%\n",
      "  erm_seed5.pt -> WGA=68.7%\n",
      "\n",
      "Debiased (DFR):\n",
      "  teacher_1_debiased.pt -> WGA=93.3%\n",
      "  teacher_2_debiased.pt -> WGA=92.1%\n",
      "  teacher_3_debiased.pt -> WGA=91.9%\n",
      "  teacher_4_debiased.pt -> WGA=93.6%\n",
      "  teacher_5_debiased.pt -> WGA=93.8%\n",
      "\n",
      "Average WGA improvement: +22.1%\n",
      "Summary saved to /content/drive/MyDrive/MIT/MIT Junior Year (2025-2026)/Fall Semester/6.7960/6.7960 Final Project/robust-ensemble-kd/teacher_checkpoints/preparation_summary.pt\n",
      "\n",
      "‚úÖ DFR complete! Debiased teachers saved.\n"
     ]
    }
   ],
   "source": [
    "# === APPLY DFR TO ALL TEACHERS ===\n",
    "from prepare_teachers import colab_prepare_teachers\n",
    "\n",
    "print(\"Applying DFR to all ERM teachers...\")\n",
    "print(f\"This will create debiased versions in: {TEACHER_DIR}\\n\")\n",
    "\n",
    "results = colab_prepare_teachers(\n",
    "    checkpoint_dir=TEACHER_DIR,\n",
    "    data_dir=DATA_DIR,\n",
    "    num_teachers=5\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ DFR complete! Debiased teachers saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuQJOaWRhYKr"
   },
   "source": [
    "# ‚úÖ Summarize Teachers\n",
    "\n",
    "Load each debiased teacher and verify WGA improved from ~73% to ~90%+."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "summary = torch.load(f'{TEACHER_DIR}/preparation_summary.pt')\n",
    "\n",
    "print(\"Teacher Results:\")\n",
    "print(\"-\" * 60)\n",
    "for seed, res in sorted(summary['results'].items()):\n",
    "    biased = res['biased']\n",
    "    debiased = res['debiased']\n",
    "    print(f\"Seed {seed}:\")\n",
    "    print(f\"  Biased:   WGA={biased['wga']*100:.1f}%, Avg={biased['avg_acc']*100:.1f}%\")\n",
    "    print(f\"  Debiased: WGA={debiased['wga']*100:.1f}%, Avg={debiased['avg_acc']*100:.1f}%\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tHMU84SOI-U",
    "outputId": "67e1c58a-4c3b-4106-c7b5-fe3d1f5796c7"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Teacher Results:\n",
      "------------------------------------------------------------\n",
      "Seed 1:\n",
      "  Biased:   WGA=73.8%, Avg=91.8%\n",
      "  Debiased: WGA=93.3%, Avg=94.4%\n",
      "Seed 2:\n",
      "  Biased:   WGA=71.2%, Avg=91.2%\n",
      "  Debiased: WGA=92.1%, Avg=94.9%\n",
      "Seed 3:\n",
      "  Biased:   WGA=69.6%, Avg=91.4%\n",
      "  Debiased: WGA=91.9%, Avg=94.9%\n",
      "Seed 4:\n",
      "  Biased:   WGA=70.9%, Avg=91.9%\n",
      "  Debiased: WGA=93.6%, Avg=94.9%\n",
      "Seed 5:\n",
      "  Biased:   WGA=68.7%, Avg=90.6%\n",
      "  Debiased: WGA=93.8%, Avg=94.7%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbY2GyjJhYKr"
   },
   "source": [
    "# üì• One-Time: Download Waterbirds Dataset\n",
    "\n",
    "**Skip this section if you've already downloaded the data.**\n",
    "\n",
    "This downloads the Waterbirds dataset (~500MB) from Stanford NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GigWSrk0hYKr"
   },
   "outputs": [],
   "source": [
    "# === DOWNLOAD WATERBIRDS (One-time only) ===\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Workaround for SSL cert issues\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Paths\n",
    "dataset_dir = f'{DRIVE_ROOT}/data'\n",
    "waterbirds_dir = f'{dataset_dir}/waterbirds_v1.0'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Download\n",
    "url = \"https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz\"\n",
    "tar_path = f'{dataset_dir}/waterbirds.tar.gz'\n",
    "\n",
    "print(\"Downloading Waterbirds dataset (~500MB)...\")\n",
    "urllib.request.urlretrieve(url, tar_path)\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# Extract\n",
    "print(\"Extracting...\")\n",
    "with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=dataset_dir)\n",
    "\n",
    "# Rename extracted folder\n",
    "extracted = f'{dataset_dir}/waterbird_complete95_forest2water2'\n",
    "if os.path.exists(extracted):\n",
    "    import shutil\n",
    "    if os.path.exists(waterbirds_dir):\n",
    "        shutil.rmtree(waterbirds_dir)\n",
    "    shutil.move(extracted, waterbirds_dir)\n",
    "\n",
    "# Cleanup\n",
    "os.remove(tar_path)\n",
    "\n",
    "# Verify\n",
    "if os.path.exists(f'{waterbirds_dir}/metadata.csv'):\n",
    "    print(f\"‚úÖ Dataset ready at: {waterbirds_dir}\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: metadata.csv not found\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}